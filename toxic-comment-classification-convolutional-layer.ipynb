{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T01:00:07.215549Z","iopub.execute_input":"2023-12-05T01:00:07.215994Z","iopub.status.idle":"2023-12-05T01:00:07.714700Z","shell.execute_reply.started":"2023-12-05T01:00:07.215958Z","shell.execute_reply":"2023-12-05T01:00:07.713218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip","metadata":{"execution":{"iopub.status.busy":"2023-12-05T01:00:07.717316Z","iopub.execute_input":"2023-12-05T01:00:07.717851Z","iopub.status.idle":"2023-12-05T01:00:09.805869Z","shell.execute_reply.started":"2023-12-05T01:00:07.717814Z","shell.execute_reply":"2023-12-05T01:00:09.804285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"train.csv\")\ndata","metadata":{"execution":{"iopub.status.busy":"2023-12-05T01:00:09.811778Z","iopub.execute_input":"2023-12-05T01:00:09.812205Z","iopub.status.idle":"2023-12-05T01:00:11.212592Z","shell.execute_reply.started":"2023-12-05T01:00:09.812167Z","shell.execute_reply":"2023-12-05T01:00:11.211401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gensim\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\n\nsentences = [text.split() for text in data['comment_text']]\nword2vec_model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)\n\n\nmax_len = 100\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(data['comment_text'])\ntext_seq = tokenizer.texts_to_sequences(data['comment_text'])\ntext_seq = pad_sequences(text_seq,maxlen=max_len)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T01:08:58.004630Z","iopub.execute_input":"2023-12-05T01:08:58.005097Z","iopub.status.idle":"2023-12-05T01:13:05.583347Z","shell.execute_reply.started":"2023-12-05T01:08:58.005069Z","shell.execute_reply":"2023-12-05T01:13:05.582444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = tokenizer.word_index\nnum_tokens = len(vocab) + 2\nfinal_embed = np.zeros((num_tokens, 300))\n\nfor word, i in vocab.items():\n    if word in word2vec_model.wv:\n        final_embed[i] = word2vec_model.wv[word]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.regularizers import l2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.datasets import make_multilabel_classification\n#LSTM and RNN\n\nembed_input = keras.layers.Input(shape=(text_seq.shape[1],))\nembed_layer = keras.layers.Embedding(num_tokens, 300, embeddings_initializer=keras.initializers.Constant(final_embed), trainable=False)(embed_input)\nspatial_dropout = keras.layers.SpatialDropout1D(0.2)(embed_layer)\nbilstm_layer = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(spatial_dropout)\nadditional_lstm_layer = keras.layers.LSTM(64, return_sequences=True)(bilstm_layer)\nglobal_pool = keras.layers.GlobalMaxPool1D()(additional_lstm_layer)\ndense_128 = keras.layers.Dense(128, kernel_regularizer=l2(0.01))(global_pool)  # Added L2 regularization\ndropout = keras.layers.Dropout(0.1)(dense_128)\nbatch_normalization = keras.layers.BatchNormalization()(dropout)\noutput = keras.layers.Dense(6, activation='sigmoid')(batch_normalization)\n\nmodel = keras.Model(inputs=embed_input, outputs=output)\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n              loss=\"binary_crossentropy\", \n              metrics=[\"acc\"])\n\n# Early stopping callback\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\ntoxicity_labels = data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n\n# Assuming 'text_seq' is your feature data and 'toxicity_labels' is your target data\nX, y = make_multilabel_classification(len(text_seq), len(toxicity_labels), random_state=42)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)  # 20% data as validation\n\n\n# Initialize the classifier with OneVsRest\nclf = OneVsRestClassifier(SGDClassifier(max_iter=1, tol=None, warm_start=True))\n\n# Prepare for real-time plotting\nplt.ion()\nfig, ax = plt.subplots()\naccuracies = []\n\n# Simulate mini-batch learning\nfor epoch in range(10):\n    for i in range(0, len(X_train), 100): # assuming batch size of 100\n        X_batch = X_train[i:i+100]\n        y_batch = y_train[i:i+100]\n        clf.partial_fit(X_batch, y_batch, classes=np.unique(y))\n\n        # Predict on the test set\n        y_pred = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        accuracies.append(accuracy)\n\n        # Update the plot\n        ax.clear()\n        ax.plot(accuracies)\n        ax.set_title('Real-Time Accuracy')\n        ax.set_xlabel('Iteration')\n        ax.set_ylabel('Accuracy')\n        plt.draw()\n        plt.pause(0.1)\n\nplt.ioff()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = text_seq\ny = data.drop(columns=['id','comment_text'],axis=1)\nprint(len(X),len(y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_X,train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(model.evaluate(test_X,test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\npreds = model.predict(test_X)\nprint(\"ROC AUC Score\",roc_auc_score(test_y,preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming 'history.history['acc']' and 'history.history['val_acc']' are lists with length equal to the number of epochs\nnum_epochs = len(history.history['acc'])\n\nplt.plot(range(1, num_epochs + 1), history.history['acc'], label='Training Accuracy')\nplt.plot(range(1, num_epochs + 1), history.history['val_acc'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.xticks(range(1, num_epochs + 1))  # Set x-ticks to correspond to the epochs\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming 'history.history['loss']' and 'history.history['val_loss']' are lists with length equal to the number of epochs\nnum_epochs = len(history.history['loss'])\n\nplt.plot(range(1, num_epochs + 1), history.history['loss'], label='Training Loss')\nplt.plot(range(1, num_epochs + 1), history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xticks(range(1, num_epochs + 1))  # Set x-ticks to correspond to the epochs\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}